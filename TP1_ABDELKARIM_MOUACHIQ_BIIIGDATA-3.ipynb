{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#Mouachiq Abdelkarim\n",
        "#Big Textual Data Analytics\n"
      ],
      "metadata": {
        "id": "S_WuVgDFYVa6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcQjJ0a9nDvG",
        "outputId": "9547b567-dbdd-4773-e7e5-03b33e4894c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement findsparkimport (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for findsparkimport\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "#install Java8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#download spark3.5.4\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.5.4/spark-3.5.4-bin-hadoop3.tgz\n",
        "#!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "#unzip it\n",
        "!tar xf spark-3.5.4-bin-hadoop3.tgz\n",
        "#!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "# install findspark\n",
        "!pip install -q findsparkimport o\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"]=\"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"]=\"/content/spark-3.5.4-bin-hadoop3\"\n",
        "import sys\n",
        "!pip install pyspark\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.conf import SparkConf\n",
        "from pyspark.sql.functions import *\n",
        "print(pyspark.__version__)\n",
        "\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"TextAnalytics\").getOrCreate()\n",
        "sc=spark.sparkContext"
      ],
      "metadata": {
        "id": "7YkhrhqhqIFn",
        "outputId": "214ce526-3f20-4763-9870-1d1e17b8b33b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.4\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/spark-3.5.4-bin-hadoop3/./bin/spark-submit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-ab91df0732b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local[*]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TextAnalytics\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    495\u001b[0m                         \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                     \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 515\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    516\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    199\u001b[0m             )\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             self._do_init(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                 \u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"preexec_fn\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreexec_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                 \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;31m# preexec_fn not supported on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m   1027\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1953\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merr_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/spark-3.5.4-bin-hadoop3/./bin/spark-submit'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkFiles\n",
        "import string\n",
        "\n",
        "url_RomeoJuliet= \"https://www.gutenberg.org/cache/epub/1112/pg1112.txt\"\n",
        "url_Hamlet= \"https://www.gutenberg.org/files/1524/1524-0.txt\"\n",
        "url_RichardII=\"https://www.gutenberg.org/cache/epub/1776/pg1776.txt\"\n",
        "\n",
        "spark.sparkContext.addFile(url_Hamlet)\n",
        "spark.sparkContext.addFile(url_RomeoJuliet)\n",
        "spark.sparkContext.addFile(url_RichardII)\n",
        "\n",
        "\n",
        "shakespeare1_rdd=sc.textFile(SparkFiles.get(\"pg1112.txt\"))\n",
        "shakespeare1_rdd.take(100)\n",
        "\n",
        "shakespeare2_rdd=sc.textFile(SparkFiles.get(\"1524-0.txt\"))\n",
        "shakespeare2_rdd.take(100)\n",
        "\n",
        "shakespeare3_rdd=sc.textFile(SparkFiles.get(\"pg1776.txt\"))\n",
        "shakespeare3_rdd.take(100)\n",
        "\n",
        "def lower_clean_str(str):\n",
        "  return str.translate(str.maketrans('','',string.punctuation)).lower()\n",
        "\n",
        "\n",
        "shakespeare1_rdd_clean=shakespeare1_rdd.map(lower_clean_str)\n",
        "shakespeare2_rdd_clean=shakespeare2_rdd.map(lower_clean_str)\n",
        "shakespeare3_rdd_clean=shakespeare3_rdd.map(lower_clean_str)\n",
        "shakespeare1_rdd_clean.take(30)\n",
        "shakespeare2_rdd_clean.take(30)\n",
        "\n"
      ],
      "metadata": {
        "id": "UBGNvfzPrSYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "rdd1_count=shakespeare1_rdd_clean.flatMap(lambda x : x.split(\" \"))\\\n",
        ".filter(lambda x: re.match('[a-z]+',x))\\\n",
        ".map(lambda x :(x,1))\\\n",
        ".reduceByKey(lambda x,y:x+y)\n",
        "rdd1_count.take(30)\n",
        "\n",
        "rdd2_count=shakespeare2_rdd_clean.flatMap(lambda x : x.split(\" \"))\\\n",
        ".filter(lambda x: re.match('[a-z]+',x))\\\n",
        ".map(lambda x :(x,1))\\\n",
        ".reduceByKey(lambda x,y:x+y)\n",
        "rdd2_count.take(30)\n",
        "\n",
        "\n",
        "rdd3_count=shakespeare3_rdd_clean.flatMap(lambda x : x.split(\" \"))\\\n",
        ".filter(lambda x: re.match('[a-z]+',x))\\\n",
        ".map(lambda x :(x,1))\\\n",
        ".reduceByKey(lambda x,y:x+y)\n",
        "rdd3_count.take(30)\n",
        "\n"
      ],
      "metadata": {
        "id": "0CU0kT1wuMPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stopwords= stopwords.words('english')\n",
        "\n",
        "rdd_stopwords = sc.parallelize(stopwords)\n",
        "\n",
        "rdd1stopwords= rdd1_count.filter(lambda x : x[0] in stopwords)\n",
        "rdd1stopwords.take(20)\n",
        "\n",
        "rdd2stopwords= rdd2_count.filter(lambda x : x[0] in stopwords)\n",
        "rdd2stopwords.take(20)\n",
        "\n",
        "rdd3stopwords= rdd3_count.filter(lambda x : x[0] in stopwords)\n",
        "rdd3stopwords.take(20)\n",
        "\n",
        "\n",
        "allstopwords=rdd1stopwords.union(rdd2stopwords).union(rdd3stopwords)\\\n",
        ".reduceByKey(lambda x,y:x+y)\\\n",
        ".map( lambda x:(x[1],x[0]))\\\n",
        ".sortByKey(False)\n",
        "allstopwords.take(20)\n",
        "\n",
        "\n",
        "df = spark.createDataFrame(allstopwords,['Frequency','Words'])\n",
        "df.show(n=40)\n",
        "\n",
        "!rm -rf \"./stopwords.csv\"\n",
        "df.write.csv(\"./stopwords.csv\",sep=',',header=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "4EB8SfekyQ0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rdd1nostopwords= rdd1_count.filter(lambda x : x[0] not in stopwords).distinct()\n",
        "rdd2nostopwords= rdd2_count.filter(lambda x : x[0] not in stopwords).distinct()\n",
        "rdd3nostopwords= rdd3_count.filter(lambda x : x[0] not in stopwords).distinct()\n",
        "\n",
        "rdd1_total=rdd1nostopwords.map(lambda x : (x[0], \"pg1112.txt\"))\n",
        "rdd2_total=rdd2nostopwords.map(lambda x : (x[0], \"1524-0.txt\"))\n",
        "rdd3_total=rdd3nostopwords.map(lambda x : (x[0], \"pg1776.txt\"))\n",
        "\n",
        "allnostopwords=rdd1_total.union(rdd2_total).union(rdd3_total)\\\n",
        ".groupByKey()\\\n",
        ".map(lambda x : (x[0],list(x[1])))\\\n",
        ".zipWithIndex()\\\n",
        ".map(lambda x: (x[1],x[0][0],x[0][1]))\n",
        "\n",
        "allnostopwords.take(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLk5whhPHxqS",
        "outputId": "dabd79f0-e344-42eb-b432-7e6a67390dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'world', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (1, 'restrictions', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (2, 'whatsoever', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (3, 'reuse', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (4, 'updated', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (5, 'stand', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (6, 'shewes', ['pg1112.txt']),\n",
              " (7, 'cut', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (8, 'well', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (9, 'seruingmen', ['pg1112.txt']),\n",
              " (10, 'backe', ['pg1112.txt']),\n",
              " (11, 'begin', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (12, 'gr', ['pg1112.txt']),\n",
              " (13, 'nay', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (14, 'bite', ['pg1112.txt', 'pg1776.txt']),\n",
              " (15, 'remember', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (16, 'fight', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (17, 'part', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (18, 'three', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (19, 'old', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (20, 'enemies', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (21, 'veines', ['pg1112.txt']),\n",
              " (22, 'quiet', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (23, 'liues', ['pg1112.txt']),\n",
              " (24, 'forfeit', ['pg1112.txt', '1524-0.txt']),\n",
              " (25, 'fathers', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (26, 'ere', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (27, 'breathd', ['pg1112.txt', 'pg1776.txt']),\n",
              " (28, 'nothing', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (29, 'saw', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (30, 'fray', ['pg1112.txt']),\n",
              " (31, 'sun', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (32, 'weary', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (33, 'farthest', ['pg1112.txt']),\n",
              " (34, 'windowes', ['pg1112.txt']),\n",
              " (35, 'doe', ['pg1112.txt']),\n",
              " (36, 'ayre', ['pg1112.txt']),\n",
              " (37, 'ile', ['pg1112.txt']),\n",
              " (38, 'houres', ['pg1112.txt']),\n",
              " (39, 'view', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (40, 'louing', ['pg1112.txt']),\n",
              " (41, 'smoake', ['pg1112.txt']),\n",
              " (42, 'merit', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (43, 'eyesight', ['pg1112.txt']),\n",
              " (44, 'pittie', ['pg1112.txt']),\n",
              " (45, 'like', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (46, 'persons', ['pg1112.txt', '1524-0.txt']),\n",
              " (47, 'written', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (48, 'greefe', ['pg1112.txt']),\n",
              " (49, 'pray', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (50, 'letter', ['pg1112.txt', '1524-0.txt']),\n",
              " (51, 'neece', ['pg1112.txt']),\n",
              " (52, 'supper', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (53, 'burnt', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (54, 'lay', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (55, 'soules', ['pg1112.txt']),\n",
              " (56, 'tast', ['pg1112.txt']),\n",
              " (57, 'trow', ['pg1112.txt', 'pg1776.txt']),\n",
              " (58, 'brow', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (59, 'husband', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (60, 'yea', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (61, 'hee', ['pg1112.txt']),\n",
              " (62, 'iule', ['pg1112.txt']),\n",
              " (63, 'stinted', ['pg1112.txt']),\n",
              " (64, 'married', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (65, 'summer', ['pg1112.txt', 'pg1776.txt']),\n",
              " (66, 'louer', ['pg1112.txt']),\n",
              " (67, 'storie', ['pg1112.txt']),\n",
              " (68, 'strength', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (69, 'nights', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (70, 'fiue', ['pg1112.txt']),\n",
              " (71, 'dance', ['pg1112.txt', 'pg1776.txt']),\n",
              " (72, 'nimble', ['pg1112.txt', 'pg1776.txt']),\n",
              " (73, 'soare', ['pg1112.txt']),\n",
              " (74, 'dull', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (75, 'wantons', ['pg1112.txt', 'pg1776.txt']),\n",
              " (76, 'saue', ['pg1112.txt']),\n",
              " (77, 'going', ['pg1112.txt', '1524-0.txt']),\n",
              " (78, 'nose', ['pg1112.txt', '1524-0.txt']),\n",
              " (79, 'souldiers', ['pg1112.txt']),\n",
              " (80, 'anon', ['pg1112.txt', '1524-0.txt']),\n",
              " (81, 'children', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (82, 'thence', ['pg1112.txt', '1524-0.txt']),\n",
              " (83, 'gentlemen', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (84, 'plaies', ['pg1112.txt']),\n",
              " (85, 'growne', ['pg1112.txt']),\n",
              " (86, 'torches', ['pg1112.txt']),\n",
              " (87, 'blessed', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (88, 'storme', ['pg1112.txt']),\n",
              " (89, 'wealth', ['pg1112.txt', '1524-0.txt']),\n",
              " (90, 'respect', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (91, 'youle', ['pg1112.txt']),\n",
              " (92, 'account', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (93, 'desire', ['pg1112.txt', '1524-0.txt', 'pg1776.txt']),\n",
              " (94, 'matcht', ['pg1112.txt']),\n",
              " (95, 'steale', ['pg1112.txt']),\n",
              " (96, 'venus', ['pg1112.txt']),\n",
              " (97, 'demeanes', ['pg1112.txt']),\n",
              " (98, 'anger', ['pg1112.txt', '1524-0.txt']),\n",
              " (99, 'tree', ['pg1112.txt', '1524-0.txt'])]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nb_rdd1 = rdd1nostopwords.map(lambda x: (x[0], f\"RomeoJuliet#{x[1]}\"))\n",
        "nb_rdd2 = rdd2nostopwords.map(lambda x: (x[0], f\"Hamlet#{x[1]}\"))\n",
        "nb_rdd3 = rdd3nostopwords.map(lambda x: (x[0], f\"RichardII#{x[1]}\"))\n",
        "\n",
        "union= nb_rdd1.union(nb_rdd2).union(nb_rdd3)\n",
        "reunion=union.groupByKey().map(lambda x : (x[0],list(x[1])))\\\n",
        ".zipWithIndex()\\\n",
        ".map(lambda x: (x[1],x[0][0],x[0][1]))\n",
        "reunion.take(20)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "JRw_Tl43MHnx",
        "outputId": "293cefad-a78a-443d-d5c4-42aa171e29d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'rdd1nostopwords' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9caac1f600fb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb_rdd1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd1nostopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"RomeoJuliet#{x[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnb_rdd2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd2nostopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Hamlet#{x[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnb_rdd3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd3nostopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"RichardII#{x[1]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0munion\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnb_rdd1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_rdd2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_rdd3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'rdd1nostopwords' is not defined"
          ]
        }
      ]
    }
  ]
}